{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "exp2-title",
            "metadata": {},
            "source": [
                "# Experiment 2 — Add Contextual Features\n",
                "\n",
                "**Goal:** Predict `final_grade` using academic, behavioral **AND** contextual features.  \n",
                "**New features added:** `internet_access`, `travel_time`, `extra_activities`  \n",
                "\n",
                "We test if environmental/contextual factors improve prediction compared to Experiment 1.  \n",
                "This is **controlled feature experimentation** — same models, same data split, more features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Imports ---\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print('All imports loaded successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-load-header",
            "metadata": {},
            "source": [
                "## Step 1 — Load Cleaned Data\n",
                "We load the same preprocessed train and test CSVs used in Experiment 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the cleaned datasets\n",
                "train_df = pd.read_csv('../datasets/train_cleaned.csv')\n",
                "test_df  = pd.read_csv('../datasets/test_cleaned.csv')\n",
                "\n",
                "print('Training data shape :', train_df.shape)\n",
                "print('Test data shape     :', test_df.shape)\n",
                "print('\\nColumns available   :', list(train_df.columns))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-features-header",
            "metadata": {},
            "source": [
                "## Step 2 — Select Features (Academic + Behavioral + Contextual)\n",
                "\n",
                "For Experiment 2, we keep all features from Experiment 1 and **add** three contextual features:\n",
                "- `internet_access` — does the student have internet at home?\n",
                "- `travel_time` — how long does it take to commute?\n",
                "- `extra_activities` — is the student involved in extra-curricular activities?\n",
                "\n",
                "**Why add these?**  \n",
                "We want to test whether environmental factors improve the model's ability to predict `final_grade`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-select-features",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Experiment 1 features (Academic + Behavioral) ---\n",
                "exp1_features = [\n",
                "    'study_hours',\n",
                "    'attendance_percentage',\n",
                "    'math_score',\n",
                "    'science_score',\n",
                "    'english_score',\n",
                "    'study_method_coaching',\n",
                "    'study_method_group study',\n",
                "    'study_method_mixed',\n",
                "    'study_method_notes',\n",
                "    'study_method_online videos',\n",
                "    'study_method_textbook'\n",
                "]\n",
                "\n",
                "# --- NEW contextual features for Experiment 2 ---\n",
                "contextual_features = [\n",
                "    'internet_access',\n",
                "    'travel_time',\n",
                "    'extra_activities'\n",
                "]\n",
                "\n",
                "# Combine both feature sets\n",
                "exp2_features = exp1_features + contextual_features\n",
                "\n",
                "X_train = train_df[exp2_features]\n",
                "X_test  = test_df[exp2_features]\n",
                "\n",
                "y_train = train_df['final_grade']\n",
                "y_test  = test_df['final_grade']\n",
                "\n",
                "print('Experiment 2 feature count :', len(exp2_features))\n",
                "print('X_train shape              :', X_train.shape)\n",
                "print('X_test  shape              :', X_test.shape)\n",
                "print('\\nNew contextual features added:', contextual_features)\n",
                "print('\\nTarget distribution (train):')\n",
                "print(y_train.value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-train-header",
            "metadata": {},
            "source": [
                "## Step 3 — Train Models\n",
                "\n",
                "We retrain the **same two classifiers** from Experiment 1 on the extended feature set:\n",
                "1. **Logistic Regression** — linear baseline model\n",
                "2. **Decision Tree** — non-linear model that captures complex patterns\n",
                "\n",
                "Using the same models allows a **fair comparison** — any change in performance is due to the new features, not a different algorithm."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-train-lr",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 1: Logistic Regression\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "print('Logistic Regression trained successfully!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-train-dt",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 2: Decision Tree\n",
                "dt_model = DecisionTreeClassifier(random_state=42)\n",
                "dt_model.fit(X_train, y_train)\n",
                "\n",
                "print('Decision Tree trained successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-eval-header",
            "metadata": {},
            "source": [
                "## Step 4 — Evaluate Models\n",
                "\n",
                "We evaluate both models using the same metrics as Experiment 1:\n",
                "- **Accuracy** — overall correct predictions\n",
                "- **Precision** — how many predicted positives are actually positive\n",
                "- **Recall** — how many actual positives are correctly identified\n",
                "- **F1 Score** — harmonic mean of precision and recall (important for imbalanced classes)\n",
                "- **Confusion Matrix** — visual breakdown of predictions vs actual"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-predict",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions on the test set\n",
                "lr_predictions = lr_model.predict(X_test)\n",
                "dt_predictions = dt_model.predict(X_test)\n",
                "\n",
                "print('Predictions generated for both models!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-eval-lr",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Logistic Regression Evaluation\n",
                "print('LOGISTIC REGRESSION — Results (Exp 2)')\n",
                "\n",
                "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
                "print(f'\\nAccuracy: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)')\n",
                "\n",
                "print('\\nClassification Report:')\n",
                "print(classification_report(y_test, lr_predictions))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-eval-dt",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision Tree Evaluation\n",
                "print('DECISION TREE — Results (Exp 2)')\n",
                "\n",
                "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
                "print(f'\\nAccuracy: {dt_accuracy:.4f} ({dt_accuracy*100:.2f}%)')\n",
                "\n",
                "print('\\nClassification Report:')\n",
                "print(classification_report(y_test, dt_predictions))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-confusion-matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrices\n",
                "grade_labels = ['f(0)', 'e(1)', 'd(2)', 'c(3)', 'b(4)', 'a(5)']\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Logistic Regression confusion matrix\n",
                "ConfusionMatrixDisplay.from_predictions(\n",
                "    y_test, lr_predictions,\n",
                "    display_labels=grade_labels,\n",
                "    cmap='Blues',\n",
                "    ax=axes[0]\n",
                ")\n",
                "axes[0].set_title('Logistic Regression (Exp 2)')\n",
                "\n",
                "# Decision Tree confusion matrix\n",
                "ConfusionMatrixDisplay.from_predictions(\n",
                "    y_test, dt_predictions,\n",
                "    display_labels=grade_labels,\n",
                "    cmap='Greens',\n",
                "    ax=axes[1]\n",
                ")\n",
                "axes[1].set_title('Decision Tree (Exp 2)')\n",
                "\n",
                "plt.suptitle('Experiment 2 — Confusion Matrices', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-compare-header",
            "metadata": {},
            "source": [
                "## Step 5 — Compare Results (Experiment 1 vs Experiment 2)\n",
                "\n",
                "Now we compare the metrics from both experiments side-by-side.  \n",
                "We use the **recorded metrics from Experiment 1** to build a comparison table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp2-compare-table",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---- Experiment 1 results (recorded from experiment_1.ipynb) ----\n",
                "exp1_lr_acc = 0.7697\n",
                "exp1_lr_f1  = 0.7698\n",
                "exp1_dt_acc = 0.6643\n",
                "exp1_dt_f1  = 0.6644\n",
                "\n",
                "# ---- Experiment 2 results ----\n",
                "lr_f1 = f1_score(y_test, lr_predictions, average='weighted')\n",
                "dt_f1 = f1_score(y_test, dt_predictions, average='weighted')\n",
                "\n",
                "# Build comparison table\n",
                "comparison = pd.DataFrame({\n",
                "    'Feature Set': [\n",
                "        'Exp 1 (Academic + Behavioral)',\n",
                "        'Exp 2 (+ Contextual)',\n",
                "        'Exp 1 (Academic + Behavioral)',\n",
                "        'Exp 2 (+ Contextual)'\n",
                "    ],\n",
                "    'Model': [\n",
                "        'Logistic Regression',\n",
                "        'Logistic Regression',\n",
                "        'Decision Tree',\n",
                "        'Decision Tree'\n",
                "    ],\n",
                "    'Accuracy': [\n",
                "        round(exp1_lr_acc, 4),\n",
                "        round(lr_accuracy, 4),\n",
                "        round(exp1_dt_acc, 4),\n",
                "        round(dt_accuracy, 4)\n",
                "    ],\n",
                "    'Weighted F1': [\n",
                "        round(exp1_lr_f1, 4),\n",
                "        round(lr_f1, 4),\n",
                "        round(exp1_dt_f1, 4),\n",
                "        round(dt_f1, 4)\n",
                "    ]\n",
                "})\n",
                "\n",
                "print('=' * 70)\n",
                "print('Experiment 1 vs Experiment 2 — Results Comparison')\n",
                "print('=' * 70)\n",
                "print(comparison.to_string(index=False))\n",
                "print('=' * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp2-observations",
            "metadata": {},
            "source": [
                "## Observations\n",
                "\n",
                "- Adding contextual features (`internet_access`, `travel_time`, `extra_activities`) allows us to test whether **environmental factors** contribute to predicting `final_grade`.\n",
                "- By comparing metrics across experiments, we can determine if the additional features provide **meaningful improvement** or just add noise.\n",
                "- **Logistic Regression** may benefit if the new features have a linear relationship with the target.\n",
                "- **Decision Tree** may capture more complex interactions between the new contextual features and existing ones.\n",
                "- This controlled experimentation approach demonstrates analytical thinking — not just random modeling."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "VectoSpace (Python 3)",
            "language": "python",
            "name": "vectospace"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
