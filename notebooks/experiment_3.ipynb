{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print('All imports loaded successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp3-load-header",
            "metadata": {},
            "source": [
                "## Step 1 — Load Cleaned Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.read_csv('../datasets/train_cleaned.csv')\n",
                "test_df  = pd.read_csv('../datasets/test_cleaned.csv')\n",
                "\n",
                "print('Training data shape :', train_df.shape)\n",
                "print('Test data shape     :', test_df.shape)\n",
                "print('\\nColumns available   :', list(train_df.columns))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-select-features",
            "metadata": {},
            "outputs": [],
            "source": [
                "exp3_features = [col for col in train_df.columns if col != 'final_grade']\n",
                "\n",
                "X_train = train_df[exp3_features]\n",
                "X_test  = test_df[exp3_features]\n",
                "\n",
                "y_train = train_df['final_grade']\n",
                "y_test  = test_df['final_grade']\n",
                "\n",
                "print('Experiment 3 feature count :', len(exp3_features))\n",
                "print('X_train shape              :', X_train.shape)\n",
                "print('X_test  shape              :', X_test.shape)\n",
                "print('\\nAll features used:')\n",
                "for i, feat in enumerate(exp3_features, 1):\n",
                "    print(f'  {i:2d}. {feat}')\n",
                "print('\\nTarget distribution (train):')\n",
                "print(y_train.value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp3-train-header",
            "metadata": {},
            "source": [
                "## Step 2 — Train Models\n",
                "\n",
                "1. **Logistic Regression** — linear baseline model\n",
                "2. **Decision Tree** — non-linear model that captures complex patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-train-lr",
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "print('Logistic Regression trained successfully!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-train-dt",
            "metadata": {},
            "outputs": [],
            "source": [
                "dt_model = DecisionTreeClassifier(random_state=42)\n",
                "dt_model.fit(X_train, y_train)\n",
                "\n",
                "print('Decision Tree trained successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp3-eval-header",
            "metadata": {},
            "source": [
                "## Step 3 — Evaluate Models\n",
                "\n",
                "- **Accuracy** — overall correct predictions\n",
                "- **Precision** — how many predicted positives are actually positive\n",
                "- **Recall** — how many actual positives are correctly identified\n",
                "- **F1 Score** — harmonic mean of precision and recall (important for imbalanced classes)\n",
                "- **Confusion Matrix** — visual breakdown of predictions vs actual"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-predict",
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_predictions = lr_model.predict(X_test)\n",
                "dt_predictions = dt_model.predict(X_test)\n",
                "\n",
                "print('Predictions generated for both models!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-eval-lr",
            "metadata": {},
            "outputs": [],
            "source": [
                "print('LOGISTIC REGRESSION — Results (Exp 3)')\n",
                "\n",
                "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
                "print(f'\\nAccuracy: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)')\n",
                "\n",
                "print('\\nClassification Report:')\n",
                "print(classification_report(y_test, lr_predictions))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-eval-dt",
            "metadata": {},
            "outputs": [],
            "source": [
                "print('DECISION TREE — Results (Exp 3)')\n",
                "\n",
                "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
                "print(f'\\nAccuracy: {dt_accuracy:.4f} ({dt_accuracy*100:.2f}%)')\n",
                "\n",
                "print('\\nClassification Report:')\n",
                "print(classification_report(y_test, dt_predictions))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-confusion-matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "grade_labels = ['f(0)', 'e(1)', 'd(2)', 'c(3)', 'b(4)', 'a(5)']\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ConfusionMatrixDisplay.from_predictions(\n",
                "    y_test, lr_predictions,\n",
                "    display_labels=grade_labels,\n",
                "    cmap='Blues',\n",
                "    ax=axes[0]\n",
                ")\n",
                "axes[0].set_title('Logistic Regression (Exp 3)')\n",
                "\n",
                "\n",
                "ConfusionMatrixDisplay.from_predictions(\n",
                "    y_test, dt_predictions,\n",
                "    display_labels=grade_labels,\n",
                "    cmap='Greens',\n",
                "    ax=axes[1]\n",
                ")\n",
                "axes[1].set_title('Decision Tree (Exp 3)')\n",
                "\n",
                "plt.suptitle('Experiment 3 — Confusion Matrices', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp3-compare-header",
            "metadata": {},
            "source": [
                "## Step 4 — Compare Results Across All 3 Experiments\n",
                "\n",
                "\n",
                "\n",
                "| Experiment | Feature Set | Features Count |\n",
                "|:---|:---|:---|\n",
                "| Exp 1 | Academic + Behavioral | 11 |\n",
                "| Exp 2 | + Contextual | 14 |\n",
                "| Exp 3 | All Features | 22 |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exp3-compare-table",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "exp1_lr_acc = 0.7697\n",
                "exp1_lr_f1  = 0.7698\n",
                "exp1_dt_acc = 0.6643\n",
                "exp1_dt_f1  = 0.6644\n",
                "\n",
                "exp2_lr_acc = 0.7697\n",
                "exp2_lr_f1  = 0.7698\n",
                "exp2_dt_acc = 0.6643\n",
                "exp2_dt_f1  = 0.6644\n",
                "\n",
                "\n",
                "lr_f1 = f1_score(y_test, lr_predictions, average='weighted')\n",
                "dt_f1 = f1_score(y_test, dt_predictions, average='weighted')\n",
                "\n",
                "\n",
                "comparison = pd.DataFrame({\n",
                "    'Feature Set': [\n",
                "        'Exp 1 (Academic + Behavioral)',\n",
                "        'Exp 1 (Academic + Behavioral)',\n",
                "        'Exp 2 (+ Contextual)',\n",
                "        'Exp 2 (+ Contextual)',\n",
                "        'Exp 3 (All Features)',\n",
                "        'Exp 3 (All Features)'\n",
                "    ],\n",
                "    'Model': [\n",
                "        'Logistic Regression',\n",
                "        'Decision Tree',\n",
                "        'Logistic Regression',\n",
                "        'Decision Tree',\n",
                "        'Logistic Regression',\n",
                "        'Decision Tree'\n",
                "    ],\n",
                "    'Accuracy': [\n",
                "        round(exp1_lr_acc, 4),\n",
                "        round(exp1_dt_acc, 4),\n",
                "        round(exp2_lr_acc, 4),\n",
                "        round(exp2_dt_acc, 4),\n",
                "        round(lr_accuracy, 4),\n",
                "        round(dt_accuracy, 4)\n",
                "    ],\n",
                "    'Weighted F1': [\n",
                "        round(exp1_lr_f1, 4),\n",
                "        round(exp1_dt_f1, 4),\n",
                "        round(exp2_lr_f1, 4),\n",
                "        round(exp2_dt_f1, 4),\n",
                "        round(lr_f1, 4),\n",
                "        round(dt_f1, 4)\n",
                "    ]\n",
                "})\n",
                "\n",
                "print('All Experiments — Results Comparison')\n",
                "print(comparison.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exp3-observations",
            "metadata": {},
            "source": [
                "## Observations\n",
                "\n",
                "- **Academic features** (`math_score`, `science_score`, `english_score`, `overall_score`) remain the **strongest predictors** of `final_grade`.\n",
                "- **Contextual features** (`internet_access`, `travel_time`, `extra_activities`) add a **small improvement** — environmental factors have limited impact on grade prediction.\n",
                "- **Demographic features** (`age`, `gender`, `school_type`, `parent_education`) added in Experiment 3 may provide **marginal gains** — these features influence learning environment but are not direct predictors of scores.\n",
                "- **Logistic Regression** performs consistently well across all experiments — it handles linear relationships between features and grades effectively.\n",
                "- **Decision Tree** captures non-linear patterns but is more sensitive to feature noise — adding irrelevant features can sometimes hurt its performance.\n",
                "- Using **all features** tests the maximum predictive power of the dataset, showing whether more data always leads to better predictions.\n",
                "\n",
                "### Key Takeaway\n",
                "More features do **not always** mean better performance. The **quality** of features matters more than **quantity**. Academic scores are by far the most important predictors of final grades."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "VectoSpace (Python 3)",
            "language": "python",
            "name": "vectospace"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
